//
// This file was generated by llmaker.
//

use std::cmp::Ordering;

use super::lexer;
use super::types::{
  self, UntypedAST,
  UntypedASTMain::*,
  UntypedFnInfo,
  UntypedVarInfo::{self, *},
};

#[derive(Debug, Clone)]
pub enum ParseError {
  UnexpectedToken(lexer::Token),
  RedundantExpression(lexer::Token),
  Eof,
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
pub fn parse(
  tokens: Vec<lexer::Token>,
) -> Result<(Vec<types::UntypedFnInfo>, types::UntypedAST), ParseError> {
  let (ret, pos) = _parse_fn_main(&tokens, 0)?;
  match pos.cmp(&tokens.len()) {
    Ordering::Equal => Ok(ret),
    Ordering::Greater => Err(ParseError::Eof), // pos > tokens.len()
    Ordering::Less => Err(ParseError::RedundantExpression(tokens[pos].clone())),
  }
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_fn_main(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<((Vec<types::UntypedFnInfo>, types::UntypedAST), usize), ParseError> {
  let mut _token_pos = pos;
  let token1 = tokens.get(pos);
  enum CodeType {
    Code0,
    Code1,
    Other,
  }
  let code_type = token1.ok_or(ParseError::Eof).map(|tok| match tok {
    (lexer::TokenKind::FALSE, _) => CodeType::Code0,
    (lexer::TokenKind::FLOATCONST(_), _) => CodeType::Code0,
    (lexer::TokenKind::FN, _) => CodeType::Code1,
    (lexer::TokenKind::IF, _) => CodeType::Code0,
    (lexer::TokenKind::INTCONST(_), _) => CodeType::Code0,
    (lexer::TokenKind::LPAREN, _) => CodeType::Code0,
    (lexer::TokenKind::TRUE, _) => CodeType::Code0,
    (lexer::TokenKind::VAR(_), _) => CodeType::Code0,

    _ => CodeType::Other,
  });
  let main = match code_type? {
    CodeType::Code0 => {
      let (utast, pos) = _parse_fn_xif(tokens, pos)?;
      let (_, pos) = _parse_token_Tok_EOF(tokens, pos)?;

      _token_pos = pos;

      (Vec::new(), utast)
    }
    CodeType::Code1 => {
      let (fn_lst, pos) = _parse_fn_fn_lst(tokens, pos)?;
      let (utast, pos) = _parse_fn_xif(tokens, pos)?;
      let (_, pos) = _parse_token_Tok_EOF(tokens, pos)?;

      _token_pos = pos;

      let mut l = fn_lst;
      l.reverse();
      (l, utast)
    }
    _ => {
      return Err(ParseError::UnexpectedToken(
        tokens.iter().next().unwrap().clone(),
      ))
    }
  };
  Ok((main, _token_pos))
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_fn_fn_lst(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(Vec<types::UntypedFnInfo>, usize), ParseError> {
  let mut _token_pos = pos;
  let token1 = tokens.get(pos);
  enum CodeType {
    Code0,
    Other,
  }
  let code_type = token1.ok_or(ParseError::Eof).map(|tok| match tok {
    (lexer::TokenKind::FN, _) => CodeType::Code0,

    _ => CodeType::Other,
  });
  let main = match code_type? {
    CodeType::Code0 => {
      let (make_fn, pos) = _parse_fn_fn(tokens, pos)?;
      let (sublst, pos) = _parse_fn_fn_lst(tokens, pos)?;

      _token_pos = pos;

      let mut l = sublst;
      l.push(make_fn);
      l
    }
    _ => Vec::new(),
  };
  Ok((main, _token_pos))
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_fn_fn(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(types::UntypedFnInfo, usize), ParseError> {
  let mut _token_pos = pos;
  let token1 = tokens.get(pos);
  enum CodeType {
    Code0,
    Other,
  }
  let code_type = token1.ok_or(ParseError::Eof).map(|tok| match tok {
    (lexer::TokenKind::FN, _) => CodeType::Code0,

    _ => CodeType::Other,
  });
  let main = match code_type? {
    CodeType::Code0 => {
      let (s, pos) = _parse_token_Tok_FN(tokens, pos)?;
      let (name, pos) = _parse_token_Tok_VAR(tokens, pos)?;
      let (_, pos) = _parse_token_Tok_LPAREN(tokens, pos)?;
      let (args, pos) = _parse_fn_var_comma_lst(tokens, pos)?;
      let (_, pos) = _parse_token_Tok_RPAREN(tokens, pos)?;
      let (_, pos) = _parse_token_Tok_LBRACKET(tokens, pos)?;
      let (vars, pos) = _parse_fn_let_var_lst(tokens, pos)?;
      let (return_var, pos) = _parse_fn_xif(tokens, pos)?;
      let (l, pos) = _parse_token_Tok_RBRACKET(tokens, pos)?;

      _token_pos = pos;

      let (_, s) = s;
      let (_, l) = l;
      let range = s.merge(&l);
      let (name_v, _) = name;
      let name = lexer::get_string(name_v).unwrap();
      types::UntypedFnInfo {
        name,
        args,
        vars,
        return_var,
        range,
      }
    }
    _ => {
      return Err(ParseError::UnexpectedToken(
        tokens.iter().next().unwrap().clone(),
      ))
    }
  };
  Ok((main, _token_pos))
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_fn_var_comma_lst(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(Vec<String>, usize), ParseError> {
  let mut _token_pos = pos;
  let token1 = tokens.get(pos);
  enum CodeType {
    Code0,
    Other,
  }
  let code_type = token1.ok_or(ParseError::Eof).map(|tok| match tok {
    (lexer::TokenKind::VAR(_), _) => CodeType::Code0,

    _ => CodeType::Other,
  });
  let main = match code_type? {
    CodeType::Code0 => {
      let (utast1, pos) = _parse_token_Tok_VAR(tokens, pos)?;
      let (utast2, pos) = _parse_fn_var_commma_lst_sub(tokens, pos)?;

      _token_pos = pos;

      let (tok, _) = utast1;
      let mut v = utast2;
      v.push(lexer::get_string(tok).unwrap());
      v
    }
    _ => Vec::new(),
  };
  Ok((main, _token_pos))
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_fn_var_commma_lst_sub(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(Vec<String>, usize), ParseError> {
  let mut _token_pos = pos;
  let token1 = tokens.get(pos);
  enum CodeType {
    Code0,
    Other,
  }
  let code_type = token1.ok_or(ParseError::Eof).map(|tok| match tok {
    (lexer::TokenKind::COMMA, _) => CodeType::Code0,

    _ => CodeType::Other,
  });
  let main = match code_type? {
    CodeType::Code0 => {
      let (_, pos) = _parse_token_Tok_COMMA(tokens, pos)?;
      let (lst, pos) = _parse_fn_var_commma_lst_sub(tokens, pos)?;

      _token_pos = pos;
      lst
    }
    _ => Vec::new(),
  };
  Ok((main, _token_pos))
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_fn_let_var_lst(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(Vec<UntypedVarInfo>, usize), ParseError> {
  let mut _token_pos = pos;
  let token1 = tokens.get(pos);
  enum CodeType {
    Code0,
    Code1,
    Other,
  }
  let code_type = token1.ok_or(ParseError::Eof).map(|tok| match tok {
    (lexer::TokenKind::LET, _) => CodeType::Code1,
    (lexer::TokenKind::VAR(_), _) => CodeType::Code0,

    _ => CodeType::Other,
  });
  let main = match code_type? {
    CodeType::Code0 => {
      let (var_name, pos) = _parse_token_Tok_VAR(tokens, pos)?;
      let (_, pos) = _parse_token_Tok_DEF_EQ(tokens, pos)?;
      let (utast, pos) = _parse_fn_xif(tokens, pos)?;
      let (_, pos) = _parse_token_Tok_SEMICOLON(tokens, pos)?;
      let (lst, pos) = _parse_fn_let_var_lst(tokens, pos)?;

      _token_pos = pos;

      let mut l = lst;
      let (var_tok, _) = var_name;
      let var_name = lexer::get_string(var_tok).unwrap();
      l.push(ReplaceVar(var_name, utast));
      l
    }
    CodeType::Code1 => {
      let (_, pos) = _parse_token_Tok_LET(tokens, pos)?;
      let (v, pos) = _parse_fn_let_var_sub(tokens, pos)?;
      let (lst, pos) = _parse_fn_let_var_lst(tokens, pos)?;

      _token_pos = pos;

      let mut l = lst;
      l.push(v);
      l
    }
    _ => Vec::new(),
  };
  Ok((main, _token_pos))
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_fn_let_var_sub(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(UntypedVarInfo, usize), ParseError> {
  let mut _token_pos = pos;
  let token1 = tokens.get(pos);
  enum CodeType {
    Code0,
    Code1,
    Other,
  }
  let code_type = token1.ok_or(ParseError::Eof).map(|tok| match tok {
    (lexer::TokenKind::MUT, _) => CodeType::Code1,
    (lexer::TokenKind::VAR(_), _) => CodeType::Code0,

    _ => CodeType::Other,
  });
  let main = match code_type? {
    CodeType::Code0 => {
      let (var_name, pos) = _parse_token_Tok_VAR(tokens, pos)?;
      let (_, pos) = _parse_token_Tok_DEF_EQ(tokens, pos)?;
      let (utast, pos) = _parse_fn_xif(tokens, pos)?;
      let (_, pos) = _parse_token_Tok_SEMICOLON(tokens, pos)?;

      _token_pos = pos;

      let (var_tok, _) = var_name;
      let var_name = lexer::get_string(var_tok).unwrap();
      Var(var_name, utast)
    }
    CodeType::Code1 => {
      let (_, pos) = _parse_token_Tok_MUT(tokens, pos)?;
      let (var_name, pos) = _parse_token_Tok_VAR(tokens, pos)?;
      let (_, pos) = _parse_token_Tok_DEF_EQ(tokens, pos)?;
      let (utast, pos) = _parse_fn_xif(tokens, pos)?;
      let (_, pos) = _parse_token_Tok_SEMICOLON(tokens, pos)?;

      _token_pos = pos;

      let (var_tok, _) = var_name;
      let var_name = lexer::get_string(var_tok).unwrap();
      VarMut(var_name, utast)
    }
    _ => {
      return Err(ParseError::UnexpectedToken(
        tokens.iter().next().unwrap().clone(),
      ))
    }
  };
  Ok((main, _token_pos))
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_fn_xif(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(types::UntypedAST, usize), ParseError> {
  let mut _token_pos = pos;
  let token1 = tokens.get(pos);
  enum CodeType {
    Code0,
    Code1,
    Other,
  }
  let code_type = token1.ok_or(ParseError::Eof).map(|tok| match tok {
    (lexer::TokenKind::FALSE, _) => CodeType::Code1,
    (lexer::TokenKind::FLOATCONST(_), _) => CodeType::Code1,
    (lexer::TokenKind::IF, _) => CodeType::Code0,
    (lexer::TokenKind::INTCONST(_), _) => CodeType::Code1,
    (lexer::TokenKind::LPAREN, _) => CodeType::Code1,
    (lexer::TokenKind::TRUE, _) => CodeType::Code1,
    (lexer::TokenKind::VAR(_), _) => CodeType::Code1,

    _ => CodeType::Other,
  });
  let main = match code_type? {
    CodeType::Code0 => {
      let (opn, pos) = _parse_token_Tok_IF(tokens, pos)?;
      let (utast1, pos) = _parse_fn_lor(tokens, pos)?;
      let (_, pos) = _parse_token_Tok_THEN(tokens, pos)?;
      let (utast2, pos) = _parse_fn_lor(tokens, pos)?;
      let (_, pos) = _parse_token_Tok_ELSE(tokens, pos)?;
      let (utast3, pos) = _parse_fn_lor(tokens, pos)?;

      _token_pos = pos;

      let (_, rngs) = opn;
      let (_, rnge) = utast3;
      let rng = rngs.merge(&rnge);
      (
        types::UntypedASTMain::IfThenElse(Box::new(utast1), Box::new(utast2), Box::new(utast3)),
        rng,
      )
    }
    CodeType::Code1 => {
      let (utast, pos) = _parse_fn_lor(tokens, pos)?;

      _token_pos = pos;
      utast
    }
    _ => {
      return Err(ParseError::UnexpectedToken(
        tokens.iter().next().unwrap().clone(),
      ))
    }
  };
  Ok((main, _token_pos))
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_fn_lor(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(types::UntypedAST, usize), ParseError> {
  let mut _token_pos = pos;
  let token1 = tokens.get(pos);
  enum CodeType {
    Code0,
    Other,
  }
  let code_type = token1.ok_or(ParseError::Eof).map(|tok| match tok {
    (lexer::TokenKind::FALSE, _) => CodeType::Code0,
    (lexer::TokenKind::FLOATCONST(_), _) => CodeType::Code0,
    (lexer::TokenKind::INTCONST(_), _) => CodeType::Code0,
    (lexer::TokenKind::LPAREN, _) => CodeType::Code0,
    (lexer::TokenKind::TRUE, _) => CodeType::Code0,
    (lexer::TokenKind::VAR(_), _) => CodeType::Code0,

    _ => CodeType::Other,
  });
  let main = match code_type? {
    CodeType::Code0 => {
      let (utast, pos) = _parse_fn_land(tokens, pos)?;
      let (utaststr2opt, pos) = _parse_fn_lor_sub(tokens, pos)?;

      _token_pos = pos;

      match utaststr2opt {
        None => utast,
        Some((op, utastr)) => {
          let (tok, rng) = op;
          let st = lexer::get_string(tok).unwrap();
          types::binary_operator(utast, (st, rng), utastr)
        }
      }
    }
    _ => {
      return Err(ParseError::UnexpectedToken(
        tokens.iter().next().unwrap().clone(),
      ))
    }
  };
  Ok((main, _token_pos))
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_fn_lor_sub(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(Option<(lexer::Token, types::UntypedAST)>, usize), ParseError> {
  let mut _token_pos = pos;
  let token1 = tokens.get(pos);
  enum CodeType {
    Code0,
    Other,
  }
  let code_type = token1.ok_or(ParseError::Eof).map(|tok| match tok {
    (lexer::TokenKind::BINOP_BAR(_), _) => CodeType::Code0,

    _ => CodeType::Other,
  });
  let main = match code_type? {
    CodeType::Code0 => {
      let (op, pos) = _parse_token_Tok_BINOP_BAR(tokens, pos)?;
      let (utastr, pos) = _parse_fn_land(tokens, pos)?;
      let (subopt, pos) = _parse_fn_lor_sub(tokens, pos)?;

      _token_pos = pos;

      match subopt {
        None => Some((op, utastr)),
        Some((op2, utastr2)) => {
          let (tok, rng) = op2;
          let st = lexer::get_string(tok).unwrap();
          Some((op, types::binary_operator(utastr, (st, rng), utastr2)))
        }
      }
    }
    _ => None,
  };
  Ok((main, _token_pos))
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_fn_land(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(types::UntypedAST, usize), ParseError> {
  let mut _token_pos = pos;
  let token1 = tokens.get(pos);
  enum CodeType {
    Code0,
    Other,
  }
  let code_type = token1.ok_or(ParseError::Eof).map(|tok| match tok {
    (lexer::TokenKind::FALSE, _) => CodeType::Code0,
    (lexer::TokenKind::FLOATCONST(_), _) => CodeType::Code0,
    (lexer::TokenKind::INTCONST(_), _) => CodeType::Code0,
    (lexer::TokenKind::LPAREN, _) => CodeType::Code0,
    (lexer::TokenKind::TRUE, _) => CodeType::Code0,
    (lexer::TokenKind::VAR(_), _) => CodeType::Code0,

    _ => CodeType::Other,
  });
  let main = match code_type? {
    CodeType::Code0 => {
      let (utast, pos) = _parse_fn_leq_lt_gt(tokens, pos)?;
      let (utaststr2opt, pos) = _parse_fn_land_sub(tokens, pos)?;

      _token_pos = pos;

      match utaststr2opt {
        None => utast,
        Some((op, utastr)) => {
          let (tok, rng) = op;
          let st = lexer::get_string(tok).unwrap();
          types::binary_operator(utast, (st, rng), utastr)
        }
      }
    }
    _ => {
      return Err(ParseError::UnexpectedToken(
        tokens.iter().next().unwrap().clone(),
      ))
    }
  };
  Ok((main, _token_pos))
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_fn_land_sub(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(Option<(lexer::Token, types::UntypedAST)>, usize), ParseError> {
  let mut _token_pos = pos;
  let token1 = tokens.get(pos);
  enum CodeType {
    Code0,
    Other,
  }
  let code_type = token1.ok_or(ParseError::Eof).map(|tok| match tok {
    (lexer::TokenKind::BINOP_AMP(_), _) => CodeType::Code0,

    _ => CodeType::Other,
  });
  let main = match code_type? {
    CodeType::Code0 => {
      let (op, pos) = _parse_token_Tok_BINOP_AMP(tokens, pos)?;
      let (utastr, pos) = _parse_fn_leq_lt_gt(tokens, pos)?;
      let (subopt, pos) = _parse_fn_land_sub(tokens, pos)?;

      _token_pos = pos;

      match subopt {
        None => Some((op, utastr)),
        Some((op2, utastr2)) => {
          let (tok, rng) = op2;
          let st = lexer::get_string(tok).unwrap();
          Some((op, types::binary_operator(utastr, (st, rng), utastr2)))
        }
      }
    }
    _ => None,
  };
  Ok((main, _token_pos))
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_fn_leq_lt_gt(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(types::UntypedAST, usize), ParseError> {
  let mut _token_pos = pos;
  let token1 = tokens.get(pos);
  enum CodeType {
    Code0,
    Other,
  }
  let code_type = token1.ok_or(ParseError::Eof).map(|tok| match tok {
    (lexer::TokenKind::FALSE, _) => CodeType::Code0,
    (lexer::TokenKind::FLOATCONST(_), _) => CodeType::Code0,
    (lexer::TokenKind::INTCONST(_), _) => CodeType::Code0,
    (lexer::TokenKind::LPAREN, _) => CodeType::Code0,
    (lexer::TokenKind::TRUE, _) => CodeType::Code0,
    (lexer::TokenKind::VAR(_), _) => CodeType::Code0,

    _ => CodeType::Other,
  });
  let main = match code_type? {
    CodeType::Code0 => {
      let (utast, pos) = _parse_fn_lhat(tokens, pos)?;
      let (utaststr2opt, pos) = _parse_fn_leq_lt_gt_sub(tokens, pos)?;

      _token_pos = pos;

      match utaststr2opt {
        None => utast,
        Some((op, utastr)) => {
          let (tok, rng) = op;
          let st = lexer::get_string(tok).unwrap();
          types::binary_operator(utast, (st, rng), utastr)
        }
      }
    }
    _ => {
      return Err(ParseError::UnexpectedToken(
        tokens.iter().next().unwrap().clone(),
      ))
    }
  };
  Ok((main, _token_pos))
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_fn_leq_lt_gt_sub(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(Option<(lexer::Token, types::UntypedAST)>, usize), ParseError> {
  let mut _token_pos = pos;
  let token1 = tokens.get(pos);
  enum CodeType {
    Code0,
    Code1,
    Code2,
    Other,
  }
  let code_type = token1.ok_or(ParseError::Eof).map(|tok| match tok {
    (lexer::TokenKind::BINOP_EQ(_), _) => CodeType::Code0,
    (lexer::TokenKind::BINOP_GT(_), _) => CodeType::Code2,
    (lexer::TokenKind::BINOP_LT(_), _) => CodeType::Code1,

    _ => CodeType::Other,
  });
  let main = match code_type? {
    CodeType::Code0 => {
      let (op, pos) = _parse_token_Tok_BINOP_EQ(tokens, pos)?;
      let (utastr, pos) = _parse_fn_lhat(tokens, pos)?;
      let (subopt, pos) = _parse_fn_leq_lt_gt_sub(tokens, pos)?;

      _token_pos = pos;

      match subopt {
        None => Some((op, utastr)),
        Some((op2, utastr2)) => {
          let (tok, rng) = op2;
          let st = lexer::get_string(tok).unwrap();
          Some((op, types::binary_operator(utastr, (st, rng), utastr2)))
        }
      }
    }
    CodeType::Code1 => {
      let (op, pos) = _parse_token_Tok_BINOP_LT(tokens, pos)?;
      let (utastr, pos) = _parse_fn_lhat(tokens, pos)?;
      let (subopt, pos) = _parse_fn_leq_lt_gt_sub(tokens, pos)?;

      _token_pos = pos;

      match subopt {
        None => Some((op, utastr)),
        Some((op2, utastr2)) => {
          let (tok, rng) = op2;
          let st = lexer::get_string(tok).unwrap();
          Some((op, types::binary_operator(utastr, (st, rng), utastr2)))
        }
      }
    }
    CodeType::Code2 => {
      let (op, pos) = _parse_token_Tok_BINOP_GT(tokens, pos)?;
      let (utastr, pos) = _parse_fn_lhat(tokens, pos)?;
      let (subopt, pos) = _parse_fn_leq_lt_gt_sub(tokens, pos)?;

      _token_pos = pos;

      match subopt {
        None => Some((op, utastr)),
        Some((op2, utastr2)) => {
          let (tok, rng) = op2;
          let st = lexer::get_string(tok).unwrap();
          Some((op, types::binary_operator(utastr, (st, rng), utastr2)))
        }
      }
    }
    _ => None,
  };
  Ok((main, _token_pos))
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_fn_lhat(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(types::UntypedAST, usize), ParseError> {
  let mut _token_pos = pos;
  let token1 = tokens.get(pos);
  enum CodeType {
    Code0,
    Other,
  }
  let code_type = token1.ok_or(ParseError::Eof).map(|tok| match tok {
    (lexer::TokenKind::FALSE, _) => CodeType::Code0,
    (lexer::TokenKind::FLOATCONST(_), _) => CodeType::Code0,
    (lexer::TokenKind::INTCONST(_), _) => CodeType::Code0,
    (lexer::TokenKind::LPAREN, _) => CodeType::Code0,
    (lexer::TokenKind::TRUE, _) => CodeType::Code0,
    (lexer::TokenKind::VAR(_), _) => CodeType::Code0,

    _ => CodeType::Other,
  });
  let main = match code_type? {
    CodeType::Code0 => {
      let (utast, pos) = _parse_fn_lpl_mi(tokens, pos)?;
      let (utaststr2opt, pos) = _parse_fn_lhat_sub(tokens, pos)?;

      _token_pos = pos;

      match utaststr2opt {
        None => utast,
        Some((op, utastr)) => {
          let (tok, rng) = op;
          let st = lexer::get_string(tok).unwrap();
          types::binary_operator(utast, (st, rng), utastr)
        }
      }
    }
    _ => {
      return Err(ParseError::UnexpectedToken(
        tokens.iter().next().unwrap().clone(),
      ))
    }
  };
  Ok((main, _token_pos))
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_fn_lhat_sub(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(Option<(lexer::Token, types::UntypedAST)>, usize), ParseError> {
  let mut _token_pos = pos;
  let token1 = tokens.get(pos);
  enum CodeType {
    Code0,
    Other,
  }
  let code_type = token1.ok_or(ParseError::Eof).map(|tok| match tok {
    (lexer::TokenKind::BINOP_HAT(_), _) => CodeType::Code0,

    _ => CodeType::Other,
  });
  let main = match code_type? {
    CodeType::Code0 => {
      let (op, pos) = _parse_token_Tok_BINOP_HAT(tokens, pos)?;
      let (utastr, pos) = _parse_fn_lpl_mi(tokens, pos)?;
      let (subopt, pos) = _parse_fn_lhat_sub(tokens, pos)?;

      _token_pos = pos;

      match subopt {
        None => Some((op, utastr)),
        Some((op2, utastr2)) => {
          let (tok, rng) = op2;
          let st = lexer::get_string(tok).unwrap();
          Some((op, types::binary_operator(utastr, (st, rng), utastr2)))
        }
      }
    }
    _ => None,
  };
  Ok((main, _token_pos))
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_fn_lpl_mi(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(types::UntypedAST, usize), ParseError> {
  let mut _token_pos = pos;
  let token1 = tokens.get(pos);
  enum CodeType {
    Code0,
    Other,
  }
  let code_type = token1.ok_or(ParseError::Eof).map(|tok| match tok {
    (lexer::TokenKind::FALSE, _) => CodeType::Code0,
    (lexer::TokenKind::FLOATCONST(_), _) => CodeType::Code0,
    (lexer::TokenKind::INTCONST(_), _) => CodeType::Code0,
    (lexer::TokenKind::LPAREN, _) => CodeType::Code0,
    (lexer::TokenKind::TRUE, _) => CodeType::Code0,
    (lexer::TokenKind::VAR(_), _) => CodeType::Code0,

    _ => CodeType::Other,
  });
  let main = match code_type? {
    CodeType::Code0 => {
      let (utast, pos) = _parse_fn_lti_div(tokens, pos)?;
      let (utaststr2opt, pos) = _parse_fn_lpl_mi_sub(tokens, pos)?;

      _token_pos = pos;

      match utaststr2opt {
        None => utast,
        Some((op, utastr)) => {
          let (tok, rng) = op;
          let st = lexer::get_string(tok).unwrap();
          types::binary_operator(utast, (st, rng), utastr)
        }
      }
    }
    _ => {
      return Err(ParseError::UnexpectedToken(
        tokens.iter().next().unwrap().clone(),
      ))
    }
  };
  Ok((main, _token_pos))
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_fn_lpl_mi_sub(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(Option<(lexer::Token, types::UntypedAST)>, usize), ParseError> {
  let mut _token_pos = pos;
  let token1 = tokens.get(pos);
  enum CodeType {
    Code0,
    Code1,
    Other,
  }
  let code_type = token1.ok_or(ParseError::Eof).map(|tok| match tok {
    (lexer::TokenKind::BINOP_MINUS(_), _) => CodeType::Code1,
    (lexer::TokenKind::BINOP_PLUS(_), _) => CodeType::Code0,

    _ => CodeType::Other,
  });
  let main = match code_type? {
    CodeType::Code0 => {
      let (op, pos) = _parse_token_Tok_BINOP_PLUS(tokens, pos)?;
      let (utastr, pos) = _parse_fn_lti_div(tokens, pos)?;
      let (subopt, pos) = _parse_fn_lpl_mi_sub(tokens, pos)?;

      _token_pos = pos;

      match subopt {
        None => Some((op, utastr)),
        Some((op2, utastr2)) => {
          let (tok, rng) = op2;
          let st = lexer::get_string(tok).unwrap();
          Some((op, types::binary_operator(utastr, (st, rng), utastr2)))
        }
      }
    }
    CodeType::Code1 => {
      let (op, pos) = _parse_token_Tok_BINOP_MINUS(tokens, pos)?;
      let (utastr, pos) = _parse_fn_lti_div(tokens, pos)?;
      let (subopt, pos) = _parse_fn_lpl_mi_sub(tokens, pos)?;

      _token_pos = pos;

      match subopt {
        None => Some((op, utastr)),
        Some((op2, utastr2)) => {
          let (tok, rng) = op2;
          let st = lexer::get_string(tok).unwrap();
          Some((op, types::binary_operator(utastr, (st, rng), utastr2)))
        }
      }
    }
    _ => None,
  };
  Ok((main, _token_pos))
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_fn_lti_div(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(types::UntypedAST, usize), ParseError> {
  let mut _token_pos = pos;
  let token1 = tokens.get(pos);
  enum CodeType {
    Code0,
    Other,
  }
  let code_type = token1.ok_or(ParseError::Eof).map(|tok| match tok {
    (lexer::TokenKind::FALSE, _) => CodeType::Code0,
    (lexer::TokenKind::FLOATCONST(_), _) => CodeType::Code0,
    (lexer::TokenKind::INTCONST(_), _) => CodeType::Code0,
    (lexer::TokenKind::LPAREN, _) => CodeType::Code0,
    (lexer::TokenKind::TRUE, _) => CodeType::Code0,
    (lexer::TokenKind::VAR(_), _) => CodeType::Code0,

    _ => CodeType::Other,
  });
  let main = match code_type? {
    CodeType::Code0 => {
      let (utast, pos) = _parse_fn_app(tokens, pos)?;
      let (utaststr2opt, pos) = _parse_fn_lti_div_sub(tokens, pos)?;

      _token_pos = pos;

      match utaststr2opt {
        None => utast,
        Some((op, utastr)) => {
          let (tok, rng) = op;
          let st = lexer::get_string(tok).unwrap();
          types::binary_operator(utast, (st, rng), utastr)
        }
      }
    }
    _ => {
      return Err(ParseError::UnexpectedToken(
        tokens.iter().next().unwrap().clone(),
      ))
    }
  };
  Ok((main, _token_pos))
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_fn_lti_div_sub(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(Option<(lexer::Token, types::UntypedAST)>, usize), ParseError> {
  let mut _token_pos = pos;
  let token1 = tokens.get(pos);
  enum CodeType {
    Code0,
    Code1,
    Other,
  }
  let code_type = token1.ok_or(ParseError::Eof).map(|tok| match tok {
    (lexer::TokenKind::BINOP_DIVIDES(_), _) => CodeType::Code1,
    (lexer::TokenKind::BINOP_TIMES(_), _) => CodeType::Code0,

    _ => CodeType::Other,
  });
  let main = match code_type? {
    CodeType::Code0 => {
      let (op, pos) = _parse_token_Tok_BINOP_TIMES(tokens, pos)?;
      let (utastr, pos) = _parse_fn_bot(tokens, pos)?;
      let (subopt, pos) = _parse_fn_lti_div_sub(tokens, pos)?;

      _token_pos = pos;

      match subopt {
        None => Some((op, utastr)),
        Some((op2, utastr2)) => {
          let (tok, rng) = op2;
          let st = lexer::get_string(tok).unwrap();
          Some((op, types::binary_operator(utastr, (st, rng), utastr2)))
        }
      }
    }
    CodeType::Code1 => {
      let (op, pos) = _parse_token_Tok_BINOP_DIVIDES(tokens, pos)?;
      let (utastr, pos) = _parse_fn_bot(tokens, pos)?;
      let (subopt, pos) = _parse_fn_lti_div_sub(tokens, pos)?;

      _token_pos = pos;

      match subopt {
        None => Some((op, utastr)),
        Some((op2, utastr2)) => {
          let (tok, rng) = op2;
          let st = lexer::get_string(tok).unwrap();
          Some((op, types::binary_operator(utastr, (st, rng), utastr2)))
        }
      }
    }
    _ => None,
  };
  Ok((main, _token_pos))
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_fn_app(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(types::UntypedAST, usize), ParseError> {
  let mut _token_pos = pos;
  let token1 = tokens.get(pos);
  enum CodeType {
    Code0,
    Code1,
    Other,
  }
  let code_type = token1.ok_or(ParseError::Eof).map(|tok| match tok {
    (lexer::TokenKind::FALSE, _) => CodeType::Code1,
    (lexer::TokenKind::FLOATCONST(_), _) => CodeType::Code1,
    (lexer::TokenKind::INTCONST(_), _) => CodeType::Code1,
    (lexer::TokenKind::LPAREN, _) => CodeType::Code1,
    (lexer::TokenKind::TRUE, _) => CodeType::Code1,
    (lexer::TokenKind::VAR(_), _) => CodeType::Code0,

    _ => CodeType::Other,
  });
  let main = match code_type? {
    CodeType::Code0 => {
      let (var, pos) = _parse_token_Tok_VAR(tokens, pos)?;
      let (_, pos) = _parse_token_Tok_LPAREN(tokens, pos)?;
      let (lst, pos) = _parse_fn_app_sub(tokens, pos)?;
      let (_, pos) = _parse_token_Tok_RPAREN(tokens, pos)?;

      _token_pos = pos;

      let (name, var_range) = var;
      let name = lexer::get_string(name).unwrap();
      let mut v = lst;
      v.reverse();
      let rng = match v.get(0) {
        None => var_range,
        Some((_, r)) => var_range.merge(r),
      };
      (types::UntypedASTMain::App(name, v.clone()), rng)
    }
    CodeType::Code1 => {
      let (v, pos) = _parse_fn_bot(tokens, pos)?;

      _token_pos = pos;
      v
    }
    _ => {
      return Err(ParseError::UnexpectedToken(
        tokens.iter().next().unwrap().clone(),
      ))
    }
  };
  Ok((main, _token_pos))
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_fn_app_sub(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(Vec<types::UntypedAST>, usize), ParseError> {
  let mut _token_pos = pos;
  let token1 = tokens.get(pos);
  enum CodeType {
    Code0,
    Other,
  }
  let code_type = token1.ok_or(ParseError::Eof).map(|tok| match tok {
    (lexer::TokenKind::FALSE, _) => CodeType::Code0,
    (lexer::TokenKind::FLOATCONST(_), _) => CodeType::Code0,
    (lexer::TokenKind::IF, _) => CodeType::Code0,
    (lexer::TokenKind::INTCONST(_), _) => CodeType::Code0,
    (lexer::TokenKind::LPAREN, _) => CodeType::Code0,
    (lexer::TokenKind::TRUE, _) => CodeType::Code0,
    (lexer::TokenKind::VAR(_), _) => CodeType::Code0,

    _ => CodeType::Other,
  });
  let main = match code_type? {
    CodeType::Code0 => {
      let (utast1, pos) = _parse_fn_xif(tokens, pos)?;
      let (utast2, pos) = _parse_fn_add_sub2(tokens, pos)?;

      _token_pos = pos;

      let mut v = utast2;
      v.push(utast1);
      v
    }
    _ => Vec::new(),
  };
  Ok((main, _token_pos))
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_fn_add_sub2(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(Vec<types::UntypedAST>, usize), ParseError> {
  let mut _token_pos = pos;
  let token1 = tokens.get(pos);
  enum CodeType {
    Code0,
    Other,
  }
  let code_type = token1.ok_or(ParseError::Eof).map(|tok| match tok {
    (lexer::TokenKind::COMMA, _) => CodeType::Code0,

    _ => CodeType::Other,
  });
  let main = match code_type? {
    CodeType::Code0 => {
      let (_, pos) = _parse_token_Tok_COMMA(tokens, pos)?;
      let (lst, pos) = _parse_fn_app_sub(tokens, pos)?;

      _token_pos = pos;
      lst
    }
    _ => Vec::new(),
  };
  Ok((main, _token_pos))
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_fn_bot(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(types::UntypedAST, usize), ParseError> {
  let mut _token_pos = pos;
  let token1 = tokens.get(pos);
  enum CodeType {
    Code0,
    Code1,
    Code2,
    Code3,
    Code4,
    Code5,
    Other,
  }
  let code_type = token1.ok_or(ParseError::Eof).map(|tok| match tok {
    (lexer::TokenKind::FALSE, _) => CodeType::Code3,
    (lexer::TokenKind::FLOATCONST(_), _) => CodeType::Code1,
    (lexer::TokenKind::INTCONST(_), _) => CodeType::Code0,
    (lexer::TokenKind::LPAREN, _) => CodeType::Code4,
    (lexer::TokenKind::TRUE, _) => CodeType::Code2,
    (lexer::TokenKind::VAR(_), _) => CodeType::Code5,

    _ => CodeType::Other,
  });
  let main = match code_type? {
    CodeType::Code0 => {
      let (inttok, pos) = _parse_token_Tok_INTCONST(tokens, pos)?;

      _token_pos = pos;

      let (tok, rng) = inttok;
      let i = lexer::get_i64(tok).unwrap();
      (types::UntypedASTMain::IntConst(i), rng)
    }
    CodeType::Code1 => {
      let (floattok, pos) = _parse_token_Tok_FLOATCONST(tokens, pos)?;

      _token_pos = pos;

      let (tok, rng) = floattok;
      let f = lexer::get_f64(tok).unwrap();
      (types::UntypedASTMain::FloatConst(f), rng)
    }
    CodeType::Code2 => {
      let (truetok, pos) = _parse_token_Tok_TRUE(tokens, pos)?;

      _token_pos = pos;

      let (_, rng) = truetok;
      (types::UntypedASTMain::BoolConst(true), rng)
    }
    CodeType::Code3 => {
      let (falsetok, pos) = _parse_token_Tok_FALSE(tokens, pos)?;

      _token_pos = pos;

      let (_, rng) = falsetok;
      (types::UntypedASTMain::BoolConst(false), rng)
    }
    CodeType::Code4 => {
      let (opn, pos) = _parse_token_Tok_LPAREN(tokens, pos)?;
      let (utast, pos) = _parse_fn_xif(tokens, pos)?;
      let (cls, pos) = _parse_token_Tok_RPAREN(tokens, pos)?;

      _token_pos = pos;

      let (_, rngo) = opn;
      let (_, rngc) = cls;
      let (main, _) = utast;
      (main, rngo.merge(&rngc))
    }
    CodeType::Code5 => {
      let (var, pos) = _parse_token_Tok_VAR(tokens, pos)?;

      _token_pos = pos;

      let (vartok, rng) = var;
      let varnm = lexer::get_string(vartok).unwrap();
      (types::UntypedASTMain::ContentOf(Vec::new(), varnm), rng)
    }
    _ => {
      return Err(ParseError::UnexpectedToken(
        tokens.iter().next().unwrap().clone(),
      ))
    }
  };
  Ok((main, _token_pos))
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_EOF(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::EOF, _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_VAR(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::VAR(_), _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_CONSTRUCTOR(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::CONSTRUCTOR(_), _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_INTCONST(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::INTCONST(_), _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_FLOATCONST(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::FLOATCONST(_), _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_DEF_EQ(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::DEF_EQ, _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_BINOP_TIMES(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::BINOP_TIMES(_), _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_BINOP_DIVIDES(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::BINOP_DIVIDES(_), _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_BINOP_PLUS(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::BINOP_PLUS(_), _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_BINOP_MINUS(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::BINOP_MINUS(_), _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_BINOP_HAT(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::BINOP_HAT(_), _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_BINOP_AMP(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::BINOP_AMP(_), _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_BINOP_BAR(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::BINOP_BAR(_), _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_BINOP_GT(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::BINOP_GT(_), _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_BINOP_LT(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::BINOP_LT(_), _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_BINOP_EQ(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::BINOP_EQ(_), _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_LPAREN(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::LPAREN, _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_RPAREN(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::RPAREN, _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_LBRACKET(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::LBRACKET, _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_RBRACKET(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::RBRACKET, _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_SEMICOLON(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::SEMICOLON, _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_COLON(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::COLON, _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_COMMA(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::COMMA, _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_TRUE(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::TRUE, _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_FALSE(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::FALSE, _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_IF(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::IF, _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_THEN(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::THEN, _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_ELSE(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::ELSE, _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_FN(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::FN, _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_LET(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::LET, _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_MUT(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::MUT, _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}

#[allow(non_camel_case_types)]
#[allow(non_snake_case)]
#[allow(unused_parens)]
#[allow(clippy::type_complexity)]
fn _parse_token_Tok_WHILE(
  tokens: &[lexer::Token],
  pos: usize,
) -> Result<(lexer::Token, usize), ParseError> {
  let token1 = tokens.get(pos);
  token1.ok_or(ParseError::Eof).and_then(|tok| match tok {
    (lexer::TokenKind::WHILE, _) => Ok((tok.clone(), pos + 1)),
    _ => Err(ParseError::UnexpectedToken(tok.clone())),
  })
}
